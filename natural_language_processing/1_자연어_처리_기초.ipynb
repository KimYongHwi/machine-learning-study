{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. 자연어 처리 기초.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOv7DYAk0mQapYYV5CqYtbc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimYongHwi/machine-learning-study/blob/main/natural_language_processing/1_%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC_%EA%B8%B0%EC%B4%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQxR2mng8ttO"
      },
      "source": [
        "## 영어 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moa1ybMx_Bht"
      },
      "source": [
        "## Tokenizer\n",
        "- 특수문자에 대한 처리\n",
        "    - 단어에 일반적으로 사용되는 알파벳, 숫자와는 다르게 특수문자는 별도의 처리가 필요\n",
        "    - 일괄적으로 단어의 특수문자를 제거하는 방법도 있지만 특수문자가 단어에 특별한 의미를 가질 때(물음표, 느낌표 등) 이를 학습에 반영시키지 못할 수도 있음\n",
        "    - 특수문자에 대한 일괄적인 제거보다는 데이터의 특성을 파악하고, 처리를 하는 것이 중요\n",
        "\n",
        "- 특정 단어에 대한 토큰 분리 방법\n",
        "    - 한 단어이지만 토큰으로 분리할 때 판단되는 문자들로 이루어진 `we're United Kingdom`등의 단어는 어떻게 분리해야 할지 선택이 필요\n",
        "    - `we're`은 한 단어이나 분리해도 단어의 의미에 별 영향을 끼치진 않지만 `United Kingdom`은 두 단어가 모여 특정 의미를 나태내기 때문에 분리하면 안됨\n",
        "    - 사용자가 단어의 특성을 고려해 토큰을 분리하는 것이 학습에 유리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0914IfxAT2d"
      },
      "source": [
        "### 단어 토큰화\n",
        "- 파이썬 내장 함수인 `split`을 활용해 단어 토큰화\n",
        "- 공백을 기준으로 단어를 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg8aO_fC--WZ",
        "outputId": "e7e08b97-0b9b-4d3b-95a2-f19db8f598cc"
      },
      "source": [
        "sentence = 'time is gold'\n",
        "tokens = [x for x in sentence.split(' ')]\n",
        "tokens"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['time', 'is', 'gold']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YqBGr0RApIo"
      },
      "source": [
        "- 토큰화는 `nltk` 패키지의 `tokenize` 모듈을 사용해 손쉽게 구현 가능\n",
        "- 단어 토큰화는 `word_tokenize()` 함수를 사용해 구현 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfrk4z-6AieQ",
        "outputId": "86cd2ce1-1bf2-4cef-8e12-2b3761f90842"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDWhA3dFA3tN",
        "outputId": "c007b9fc-4a76-4315-f423-df89519b780c"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['time', 'is', 'gold']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3F03-yLBE9g"
      },
      "source": [
        "### 문장 토큰화\n",
        "- 문장 토큰화는 줄바꿈 문자 `\\n`을 기준으로 문장을 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHzNwKstBC3T",
        "outputId": "905f1d8b-3d23-416a-ea50-911894d35da2"
      },
      "source": [
        "sentences = 'The world is a beautiful book.\\nBut of little use him who cannot read it.'\n",
        "\n",
        "tokens = [x for x in sentences.split('\\n')]\n",
        "tokens"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The world is a beautiful book.', 'But of little use him who cannot read it.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfPlx6X9Blrn"
      },
      "source": [
        "- 문장 토큰화는 `sent_tokenize`메소드를 사용해 구현 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLK0-Y0NBc5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1800ee0e-f9c0-445d-8045-e4389eb47a97"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "tokens = sent_tokenize(sentences)\n",
        "tokens"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The world is a beautiful book.', 'But of little use him who cannot read it.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_-oO_mnBsVA"
      },
      "source": [
        "- 문장 토큰화에서는 온점(.)의 처리를 위해 이진 분류기를 사용할 수도 있음\n",
        "- 온점은 문장과 문장을 구분해줄 수도, 문장에 포함된 단어를 구성할 수도 있기 때문에 이를 이진분류기로 분류해 더욱 좋은 토큰화를 구현할 수도 있음 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfX_htN29sES"
      },
      "source": [
        "### 정규 표현식을 이용한 토큰화\n",
        "- 토큰화 기능을 직럽 구현할 수도 있지만 정규표현식을 이용해 간단하게 구현할 수도 있음\n",
        "- nltk 패키지는 정규표현식을 사용하는 토큰화 도구인 `RegexpTokenizer`를 제공"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valEIEXG9qQ6",
        "outputId": "4f529e5a-de8f-4ba1-da1b-b60866742b28"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "sentence = 'Where there\\'s a will, there\\'s a way'\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDAN_AGT-D23",
        "outputId": "5232c222-baff-4c07-9971-ad85347d62f2"
      },
      "source": [
        "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En2xjln4_BiW"
      },
      "source": [
        "### 케라스를 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p02pSg7I-zWz",
        "outputId": "14ab55ad-4262-4a35-d437-4c1205028142"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "sentence = 'Where there\\'s a wiil, there\\'s a way'\n",
        "\n",
        "text_to_word_sequence(sentence)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['where', \"there's\", 'a', 'wiil', \"there's\", 'a', 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5H9UpDa_dFN"
      },
      "source": [
        "### TextBlob을 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sqvRkh3_M6Y",
        "outputId": "4f2149a9-485e-4525-8ade-8b9c4096d4c5"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentence = 'Where there\\'s a wiil, there\\'s a way'\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "blob.words"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Where', 'there', \"'s\", 'a', 'wiil', 'there', \"'s\", 'a', 'way'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYwrVjE8_vLv"
      },
      "source": [
        "### 기타 Tokenizer\n",
        "- WhiteSpaceTokenizer: 공백을 기준으로 토큰화\n",
        "- WordPunktTokenizer: 텍스트를 알파벳 문자, 숫자, 알파벳 이외의 문자 리스트로 토큰화\n",
        "- MWETokenizer: MWE는 Multi-Word Expression의 약자로 `republic of korea`와 같이 여러 단어로 이뤄진 특정 그룹은 한 개체로 취급\n",
        "- TweetTokenizer: 트위터에서 사용되는 문장의 토큰화를 위해서 만들어졌으며, 문장 속 감성의 표현과 감정을 다룸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLRBl3qdAWVy"
      },
      "source": [
        "### n-gram 추출\n",
        "- n-gram은 n개의 어절이나 음절을 연쇄적으로 분류해 그 빈도를 분석\n",
        "- n=1일 때는 unigram, n=2일 때는 bigram, n=3일 때는 trigram으로 불림"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ76kITE_qWy",
        "outputId": "798e5af3-5a2a-4e64-c805-2bccbae95bcd"
      },
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "sentence = 'There is no royal road to leanging'\n",
        "bigram = list(ngrams(sentence.split(), 2))\n",
        "print(bigram)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'road'), ('road', 'to'), ('to', 'leanging')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3tJpS4zBVUz",
        "outputId": "d4202661-797a-47ed-fed6-493e0dffdfba"
      },
      "source": [
        "trigram = list(ngrams(sentence.split(), 3))\n",
        "print(trigram)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('There', 'is', 'no'), ('is', 'no', 'royal'), ('no', 'royal', 'road'), ('royal', 'road', 'to'), ('road', 'to', 'leanging')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx51kDSPBhOZ",
        "outputId": "a072f6ea-2dde-44da-872f-c330d07f7fa3"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "blob.ngrams(n=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['There', 'is']),\n",
              " WordList(['is', 'no']),\n",
              " WordList(['no', 'royal']),\n",
              " WordList(['royal', 'road']),\n",
              " WordList(['road', 'to']),\n",
              " WordList(['to', 'leanging'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JthDA7tQBwco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff53f6e2-799a-4f67-f4f7-8082c250f27e"
      },
      "source": [
        "blob.ngrams(n=3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['There', 'is', 'no']),\n",
              " WordList(['is', 'no', 'royal']),\n",
              " WordList(['no', 'royal', 'road']),\n",
              " WordList(['royal', 'road', 'to']),\n",
              " WordList(['road', 'to', 'leanging'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGEB-UCkzhAK"
      },
      "source": [
        "### PoS(Parts of Speech) 태깅\n",
        "- PoS는 품사를 의미하며, PoS 태깅은 문장 내에서 단어에 해당하는 각 품사를 태깅\n",
        "- PoS 태그 리스트\n",
        "\n",
        "| Number | Tag | Description | 설명 |\n",
        "| -- | -- | -- | -- |\n",
        "| 1 | `CC` | Coordinating conjunction |\n",
        "| 2 | `CD` | Cardinal number |\n",
        "| 3 | `DT` | Determiner | 한정사\n",
        "| 4 | `EX` | Existential there |\n",
        "| 5 | `FW` | Foreign word | 외래어 |\n",
        "| 6 | `IN` | Preposition or subordinating conjunction | 전치사 또는 종속 접속사 |\n",
        "| 7 | `JJ` | Adjective | 형용사 |\n",
        "| 8 | `JJR` | Adjective, comparative | 헝용사, 비교급 |\n",
        "| 9 | `JJS` | Adjective, superlative | 형용사, 최상급 |\n",
        "| 10 | `LS` | List item marker |\n",
        "| 11 | `MD` | Modal |\n",
        "| 12 | `NN` | Noun, singular or mass | 명사, 단수형 |\n",
        "| 13 | `NNS` | Noun, plural | 명사, 복수형 |\n",
        "| 14 | `NNP` | Proper noun, singular | 고유명사, 단수형 |\n",
        "| 15 | `NNPS` | Proper noun, plural | 고유명사, 복수형 |\n",
        "| 16 | `PDT` | Predeterminer | 전치한정사 |\n",
        "| 17 | `POS` | Possessive ending | 소유형용사 |\n",
        "| 18 | `PRP` | Personal pronoun | 인칭 대명사 |\n",
        "| 19 | `PRP$` | Possessive pronoun | 소유 대명사 |\n",
        "| 20 | `RB` | Adverb | 부사 |\n",
        "| 21 | `RBR` | Adverb, comparative | 부사, 비교급 |\n",
        "| 22 | `RBS` | Adverb, superlative | 부사, 최상급 |\n",
        "| 23 | `RP` | Particle |\n",
        "| 24 | `SYM` | Symbol | 기호\n",
        "| 25 | `TO` | to |\n",
        "| 26 | `UH` | Interjection | 감탄사 |\n",
        "| 27 | `VB` | Verb, base form | 동사, 원형 |\n",
        "| 28 | `VBD` | Verb, past tense | 동사, 과거형 |\n",
        "| 29 | `VBG` | Verb, gerund or present participle | 동사, 현재분사 |\n",
        "| 30 | `VBN` | Verb, past participle | 동사, 과거분사 |\n",
        "| 31 | `VBP` | Verb, non-3rd person singular present | 동사, 비3인칭 단수 |\n",
        "| 32 | `VBZ` | Verb, 3rd person singular present | 동사, 3인칭 단수 |\n",
        "| 33 | `WDT` | Wh-determiner |\n",
        "| 34 | `WP` | Wh-pronoun |\n",
        "| 35 | `WP$` | Possessive wh-pronoun |\n",
        "| 36 | `WRB` | Wh-adverb |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4leeeJzBzAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa1854f-f3ce-4b01-8f99-dd8cc7bb7b3f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ6-7NQ8z_J6",
        "outputId": "a5feff89-0551-4753-c1f7-23185c771deb"
      },
      "source": [
        "words = word_tokenize('Think like man of action and act like man of thought.')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(words)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Think', 'VBP'),\n",
              " ('like', 'IN'),\n",
              " ('man', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('action', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('act', 'NN'),\n",
              " ('like', 'IN'),\n",
              " ('man', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('thought', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0cGLnMV0HfH",
        "outputId": "8980bd5d-cf11-4a99-d277-29993dd65b9d"
      },
      "source": [
        "nltk.pos_tag(word_tokenize('A rolling stone gathers no moss'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 'DT'),\n",
              " ('rolling', 'VBG'),\n",
              " ('stone', 'NN'),\n",
              " ('gathers', 'NNS'),\n",
              " ('no', 'DT'),\n",
              " ('moss', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoIoRk5n0_GM"
      },
      "source": [
        "### 불용어 제거\n",
        "- 영어의 전치사(on, in), 한국어의 조서(을, 를) 등은 분석에 필요하지 않은 경우가 많음\n",
        "- 길이가 짧은 단어, 등장 빈도 수가 적은 단어들도 분석에 큰 영향을 주지 않음\n",
        "- 일반적으로 사용되는 도구들은 해당 단어들을 제거해주지만 완변하게 제거되지는 않음\n",
        "- 사용자가 불용어 사전을 만들어 해당 단어들을 제거하는 것이 좋음\n",
        "- 도구들이 걸러주지 않는 전치사, 조사 등을 불용어 사전을 만들어 불필요한 단어들을 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MHoo6_K0TNH"
      },
      "source": [
        "stop_words = \"on in the\"\n",
        "stop_words = stop_words.split(' ')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-33w-dRV1reJ",
        "outputId": "b0f186f3-8dae-4041-e0a4-80e8e5a65bb4"
      },
      "source": [
        "sentence = \"singer on the stage\"\n",
        "sentence = sentence.split(' ')\n",
        "nouns = []\n",
        "\n",
        "for noun in sentence:\n",
        "    if noun not in stop_words:\n",
        "        nouns.append(noun)\n",
        "\n",
        "nouns"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['singer', 'stage']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRhr38wW1_Hr"
      },
      "source": [
        "- nltk 패키지 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7cNNirS17xN",
        "outputId": "7b382fec-7f50-486d-8583-57578f94d83d"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMMhdxPQ2H7q",
        "outputId": "4a422dd5-22de-4232-d35f-77df951edd62"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lTQ8wcO2MsX",
        "outputId": "00465ab4-cf47-46d5-e2c4-db5ec3eaee14"
      },
      "source": [
        "s = \"If you do not walk today, you will have to run tomorrow\"\n",
        "words = word_tokenize(s)\n",
        "words"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['If',\n",
              " 'you',\n",
              " 'do',\n",
              " 'not',\n",
              " 'walk',\n",
              " 'today',\n",
              " ',',\n",
              " 'you',\n",
              " 'will',\n",
              " 'have',\n",
              " 'to',\n",
              " 'run',\n",
              " 'tomorrow']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTnDjZJT2iav",
        "outputId": "e14fe3b1-6ccf-4fb7-d2c5-e7d48b8e5a55"
      },
      "source": [
        "no_stopwords = []\n",
        "for w in words:\n",
        "    if w not in stop_words:\n",
        "        no_stopwords.append(w)\n",
        "\n",
        "print(no_stopwords)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['If', 'walk', 'today', ',', 'run', 'tomorrow']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbYZCyNM2vT5"
      },
      "source": [
        "### 철자 교정\n",
        "- 텍스트에 오탈자가 존재하는 경우가 있음\n",
        "- 사람이 적절한 추정을 통해 이해하는데는 문제가 없지만, 컴퓨터는 이러한 단어를 그대로 받아들이기 때문에 전처리가 필요\n",
        "- 철자 교정 알고리즘은 이미 개발되어 워드 프로세서나 다양한 서비스에서 많이 적용됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNjGov2Z2rqU",
        "outputId": "3d4fe122-c53f-46c5-eec0-dd7b34a75232"
      },
      "source": [
        "!pip install autocorrect\n",
        "from autocorrect import Speller"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/31/aa5d4b54baafed2d0eef47e30d527ea60eb7357f11c3b5adc58262a3c693/autocorrect-2.5.0.tar.gz (622kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 4.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.5.0-cp37-none-any.whl size=621854 sha256=ee336e69dd17306b48d44733cd3f6d8f3017dc8f3908b7339e014ec1f90b126b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/71/da/4a15028d25fbd5fb97fb76c5f76f0ad86f0caa69394dd7cfa7\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtotRtdH31be",
        "outputId": "87d6207e-7252-4c79-99c4-a0137480f32f"
      },
      "source": [
        "spell = Speller('en')\n",
        "print(spell(\"peoplle\"))\n",
        "print(spell('peope'))\n",
        "print(spell('peopae'))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "people\n",
            "people\n",
            "people\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHjcdmxZ3_xJ",
        "outputId": "005369c1-6bf6-49f3-9390-41e8c2fa854c"
      },
      "source": [
        "s = word_tokenize('Earlly biird catchess the womm.')\n",
        "print(s)\n",
        "ss = ' '.join([spell(s) for s in s])\n",
        "print(ss)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Earlly', 'biird', 'catchess', 'the', 'womm', '.']\n",
            "Early bird catches the worm .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhxyskW84XNw"
      },
      "source": [
        "#### 언어의 단수화 복수화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVcxxgfV4Uns",
        "outputId": "672a4006-b4bb-4e59-a08f-a850e1ad81ec"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "words = 'apples bananas oranges'\n",
        "tb = TextBlob(words)\n",
        "\n",
        "print(tb.words)\n",
        "print(tb.words.singularize()) # 단수화"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apples', 'bananas', 'oranges']\n",
            "['apple', 'banana', 'orange']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phRf7It-4nGB",
        "outputId": "7bbe5dc3-faac-4890-eec3-3beb65066b22"
      },
      "source": [
        "words = 'car train airplane'\n",
        "tb = TextBlob(words)\n",
        "\n",
        "print(tb.words)\n",
        "print(tb.words.pluralize())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['car', 'train', 'airplane']\n",
            "['cars', 'trains', 'airplanes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxVES_ZF48V5"
      },
      "source": [
        "### 어간(Stemming) 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDul4Wtb459J"
      },
      "source": [
        "import nltk\n",
        "\n",
        "stemmer = nltk.stem.PorterStemmer()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sA9QZF1X5FXp",
        "outputId": "ffa691c8-a212-4a47-d6c8-c700100c39bf"
      },
      "source": [
        "stemmer.stem('application')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'applic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7SLwM1d65Hf9",
        "outputId": "7284b297-7a53-4cc6-f42c-2d13f4a3a1aa"
      },
      "source": [
        "stemmer.stem('beginning')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'begin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vkJVWKYf5Lno",
        "outputId": "39592cea-f7b8-4935-e37e-7fe63a548ad2"
      },
      "source": [
        "stemmer.stem('catches')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'catch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yUi5MSDF5NfH",
        "outputId": "f98593c4-a43a-4c22-ebd7-b0309a60b77c"
      },
      "source": [
        "stemmer.stem('education')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'educ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkqDm6yc5Q9r"
      },
      "source": [
        "### 표제어(Lemmatization) 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrOHxV9G5Pcp",
        "outputId": "3becf94a-2e8a-4cd7-e5c6-b274f6119d96"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qOJt30jf5c6I",
        "outputId": "b0ce10c3-e8cf-42ba-a9d6-3c57ad535bfd"
      },
      "source": [
        "lemmatizer.lemmatize('application')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'application'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DbqNhw295kXB",
        "outputId": "33370693-a5ec-42d4-e1d4-19574ef139a9"
      },
      "source": [
        "lemmatizer.lemmatize('beginning')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'beginning'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EuWZbRb_5qTi",
        "outputId": "e0029227-4fa1-468b-ee6a-8c537c75cceb"
      },
      "source": [
        "lemmatizer.lemmatize('catches')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'catch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_-7fLRyK5sWa",
        "outputId": "75e0af70-103f-46e9-e59e-abddfbc7d638"
      },
      "source": [
        "lemmatizer.lemmatize('education')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'education'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR8a4PlN5yHI"
      },
      "source": [
        "### 개체명 인식(Named Entity Recognition)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SX5_hl35wIP",
        "outputId": "d1b8e9d1-d68c-4073-caca-62795f484aa2"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyl1sTT56D12",
        "outputId": "63173265-f9ce-4fea-ab91-fb0eee581f5d"
      },
      "source": [
        "s = \"Rome was not built in a day\"\n",
        "print(s)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rome was not built in a day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j3Hp4AR6MLy",
        "outputId": "e99cb780-f254-4da0-ace5-d5cb572e5c63"
      },
      "source": [
        "tags = nltk.pos_tag(word_tokenize(s))\n",
        "print(tags)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Rome', 'NNP'), ('was', 'VBD'), ('not', 'RB'), ('built', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('day', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g_-aYfC6PDC",
        "outputId": "2f6d4a6f-9f83-4b9a-bc5f-42608eb05b63"
      },
      "source": [
        "entities = nltk.ne_chunk(tags, binary=True)\n",
        "print(entities)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S (NE Rome/NNP) was/VBD not/RB built/VBN in/IN a/DT day/NN)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOFn87wm6Z-q"
      },
      "source": [
        "### 단어의 중의성(Lexical Ambiguity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTWMUQPL6VkO",
        "outputId": "97c0bf4b-53ca-4162-de39-fa98f21a265e"
      },
      "source": [
        "from nltk.wsd import lesk\n",
        "\n",
        "s = \"I saw bats.\" # 중의성 이 있는 문장 (나는 박쥐를 보았다, 나는 야구방망이를 보았다 등등)\n",
        "\n",
        "print(word_tokenize(s))\n",
        "print(lesk(word_tokenize(s), 'saw'))\n",
        "print(lesk(word_tokenize(s), 'bats'))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'saw', 'bats', '.']\n",
            "Synset('saw.v.01')\n",
            "Synset('squash_racket.n.01')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7dzm-SR7HVg"
      },
      "source": [
        "## 한국어 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi_kfPW57X2C"
      },
      "source": [
        "### 정규표현식\n",
        "- 한국어 정규 표션식도 대부분의 문법은 영어 정규 표현식과 같음\n",
        "- 한국어는 자음과 모음이 분리되기 때문에, 문법을 지정할 때는 자음과 모음을 동시에 고려해야함\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE_fxrfh7pls"
      },
      "source": [
        "#### match\n",
        "- 컴파일한 정규 표현식을 이용해 문자열이 정규 표현식과 맞는지 검사\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiNIJsQX7CH5",
        "outputId": "8b5c7ac2-6a52-4c21-a078-a863540c40df"
      },
      "source": [
        "import re\n",
        "\n",
        "check = '[ㄱ-ㅎ]+'\n",
        "\n",
        "print(re.match(check, 'ㅎ 안녕하세요.'))\n",
        "print(re.match(check, '안녕하세요. ㅎ'))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(0, 1), match='ㅎ'>\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4rpZcxa8Lcz"
      },
      "source": [
        "#### search\n",
        "- match와 다르게, search는 문자열의 전체를 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2SxuzMy8AWP",
        "outputId": "7b885a6a-3943-4509-8516-26ff8a8007ea"
      },
      "source": [
        "check = '[ㄱ-ㅎ|ㅏ-ㅣ]+'\n",
        "\n",
        "print(re.search(check, 'ㄱㅏ 안녕하세요.'))\n",
        "print(re.match(check, '안 ㄱㅏ'))\n",
        "print(re.search(check, '안 ㄱㅏ'))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(0, 2), match='ㄱㅏ'>\n",
            "None\n",
            "<re.Match object; span=(2, 4), match='ㄱㅏ'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqpRb-yC8m6u"
      },
      "source": [
        "#### sub\n",
        "- 정규 표현식과 일치하는 부분을 다른 문자열로 교체"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyW0gQ778heH",
        "outputId": "eac45937-ea39-4d44-a206-8e141e3d616b"
      },
      "source": [
        "print(re.sub('[가-힣]', '가나다라마바사', '1'))\n",
        "print(re.sub('[^가-힣]', '가나다라마바사', '1'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "가나다라마바사\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_-Wk3Pu9BXW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}