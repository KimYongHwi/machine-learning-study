{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. 자연어 처리 기초.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNkQa1KPGHbTvPnP/hW37Em",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimYongHwi/machine-learning-study/blob/main/natural_language_processing/1_%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC_%EA%B8%B0%EC%B4%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moa1ybMx_Bht"
      },
      "source": [
        "## Tokenizer\n",
        "- 특수문자에 대한 처리\n",
        "    - 단어에 일반적으로 사용되는 알파벳, 숫자와는 다르게 특수문자는 별도의 처리가 필요\n",
        "    - 일괄적으로 단어의 특수문자를 제거하는 방법도 있지만 특수문자가 단어에 특별한 의미를 가질 때(물음표, 느낌표 등) 이를 학습에 반영시키지 못할 수도 있음\n",
        "    - 특수문자에 대한 일괄적인 제거보다는 데이터의 특성을 파악하고, 처리를 하는 것이 중요\n",
        "\n",
        "- 특정 단어에 대한 토큰 분리 방법\n",
        "    - 한 단어이지만 토큰으로 분리할 때 판단되는 문자들로 이루어진 `we're United Kingdom`등의 단어는 어떻게 분리해야 할지 선택이 필요\n",
        "    - `we're`은 한 단어이나 분리해도 단어의 의미에 별 영향을 끼치진 않지만 `United Kingdom`은 두 단어가 모여 특정 의미를 나태내기 때문에 분리하면 안됨\n",
        "    - 사용자가 단어의 특성을 고려해 토큰을 분리하는 것이 학습에 유리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0914IfxAT2d"
      },
      "source": [
        "### 단어 토큰화\n",
        "- 파이썬 내장 함수인 `split`을 활용해 단어 토큰화\n",
        "- 공백을 기준으로 단어를 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg8aO_fC--WZ",
        "outputId": "5562eec0-a64d-437e-9ff5-f90fe48f3092"
      },
      "source": [
        "sentence = 'time is gold'\n",
        "tokens = [x for x in sentence.split(' ')]\n",
        "tokens"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['time', 'is', 'gold']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YqBGr0RApIo"
      },
      "source": [
        "- 토큰화는 `nltk` 패키지의 `tokenize` 모듈을 사용해 손쉽게 구현 가능\n",
        "- 단어 토큰화는 `word_tokenize()` 함수를 사용해 구현 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfrk4z-6AieQ",
        "outputId": "cf96abf2-d88e-4b6b-e89b-1d6b24cf1f4b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDWhA3dFA3tN",
        "outputId": "97f46073-0368-4c0c-e511-285217b8721a"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['time', 'is', 'gold']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3F03-yLBE9g"
      },
      "source": [
        "### 문장 토큰화\n",
        "- 문장 토큰화는 줄바꿈 문자 `\\n`을 기준으로 문장을 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHzNwKstBC3T",
        "outputId": "7a737ce2-1caa-40fa-e81c-d289e4ebe0ad"
      },
      "source": [
        "sentences = 'The world is a beautiful book.\\nBut of little use him who cannot read it.'\n",
        "\n",
        "tokens = [x for x in sentences.split('\\n')]\n",
        "tokens"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The world is a beautiful book.', 'But of little use him who cannot read it.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfPlx6X9Blrn"
      },
      "source": [
        "- 문장 토큰화는 `sent_tokenize`메소드를 사용해 구현 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLK0-Y0NBc5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b1ac1c-0bd2-4cce-cfb8-b8b44ff7d705"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "tokens = sent_tokenize(sentences)\n",
        "tokens"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The world is a beautiful book.', 'But of little use him who cannot read it.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_-oO_mnBsVA"
      },
      "source": [
        "- 문장 토큰화에서는 온점(.)의 처리를 위해 이진 분류기를 사용할 수도 있음\n",
        "- 온점은 문장과 문장을 구분해줄 수도, 문장에 포함된 단어를 구성할 수도 있기 때문에 이를 이진분류기로 분류해 더욱 좋은 토큰화를 구현할 수도 있음 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfX_htN29sES"
      },
      "source": [
        "### 정규 표현식을 이용한 토큰화\n",
        "- 토큰화 기능을 직럽 구현할 수도 있지만 정규표현식을 이용해 간단하게 구현할 수도 있음\n",
        "- nltk 패키지는 정규표현식을 사용하는 토큰화 도구인 `RegexpTokenizer`를 제공"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valEIEXG9qQ6",
        "outputId": "1d62a0db-c42b-4d34-aece-56cf1e129b74"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "sentence = 'Where there\\'s a will, there\\'s a way'\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDAN_AGT-D23",
        "outputId": "4d10fde6-0250-42e0-c2b4-ae15b48c5fec"
      },
      "source": [
        "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En2xjln4_BiW"
      },
      "source": [
        "### 케라스를 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p02pSg7I-zWz",
        "outputId": "e39ceeed-9696-4b47-9a37-21b49bf9be82"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "sentence = 'Where there\\'s a wiil, there\\'s a way'\n",
        "\n",
        "text_to_word_sequence(sentence)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['where', \"there's\", 'a', 'wiil', \"there's\", 'a', 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5H9UpDa_dFN"
      },
      "source": [
        "### TextBlob을 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sqvRkh3_M6Y",
        "outputId": "8bc185a8-5166-42f8-8ee9-6fe7c2f0b375"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentence = 'Where there\\'s a wiil, there\\'s a way'\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "blob.words"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Where', 'there', \"'s\", 'a', 'wiil', 'there', \"'s\", 'a', 'way'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYwrVjE8_vLv"
      },
      "source": [
        "### 기타 Tokenizer\n",
        "- WhiteSpaceTokenizer: 공백을 기준으로 토큰화\n",
        "- WordPunktTokenizer: 텍스트를 알파벳 문자, 숫자, 알파벳 이외의 문자 리스트로 토큰화\n",
        "- MWETokenizer: MWE는 Multi-Word Expression의 약자로 `republic of korea`와 같이 여러 단어로 이뤄진 특정 그룹은 한 개체로 취급\n",
        "- TweetTokenizer: 트위터에서 사용되는 문장의 토큰화를 위해서 만들어졌으며, 문장 속 감성의 표현과 감정을 다룸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLRBl3qdAWVy"
      },
      "source": [
        "### n-gram 추출\n",
        "- n-gram은 n개의 어절이나 음절을 연쇄적으로 분류해 그 빈도를 분석\n",
        "- n=1일 때는 unigram, n=2일 때는 bigram, n=3일 때는 trigram으로 불림"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ76kITE_qWy",
        "outputId": "fb01598b-472b-4c22-9e21-a049fa0e9c96"
      },
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "sentence = 'There is no royal road to leanging'\n",
        "bigram = list(ngrams(sentence.split(), 2))\n",
        "print(bigram)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'road'), ('road', 'to'), ('to', 'leanging')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3tJpS4zBVUz",
        "outputId": "1cdbc8c4-5412-410b-977f-69dccca6f277"
      },
      "source": [
        "trigram = list(ngrams(sentence.split(), 3))\n",
        "print(trigram)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('There', 'is', 'no'), ('is', 'no', 'royal'), ('no', 'royal', 'road'), ('royal', 'road', 'to'), ('road', 'to', 'leanging')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx51kDSPBhOZ",
        "outputId": "2ea9d76e-bae3-4a0b-e25f-6244a127d553"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "blob.ngrams(n=2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['There', 'is']),\n",
              " WordList(['is', 'no']),\n",
              " WordList(['no', 'royal']),\n",
              " WordList(['royal', 'road']),\n",
              " WordList(['road', 'to']),\n",
              " WordList(['to', 'leanging'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JthDA7tQBwco",
        "outputId": "2b73e9e9-d592-4c40-b912-9ab371273567"
      },
      "source": [
        "blob.ngrams(n=3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['There', 'is', 'no']),\n",
              " WordList(['is', 'no', 'royal']),\n",
              " WordList(['no', 'royal', 'road']),\n",
              " WordList(['royal', 'road', 'to']),\n",
              " WordList(['road', 'to', 'leanging'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4leeeJzBzAd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}