{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13. 개체명 인식 Named Entity Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEuG+eLRuLIc3WD+ZOuAPV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimYongHwi/machine-learning-study/blob/main/natural_language_processing/13_%EA%B0%9C%EC%B2%B4%EB%AA%85_%EC%9D%B8%EC%8B%9D_Named_Entity_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6IqCSWn6ls"
      },
      "source": [
        "### 개체명 인직(Named Entity Recognition)\n",
        "\n",
        "- 개체명 인식은 텍스트에서 이름을 가진 개체를 인식하는 기술\n",
        "- 가령 '철수와 영희는 밥을 먹었다'에서 이름과 사물을 추출하는 개체명 인식 모델 결과는 아래와 같다\n",
        "  \n",
        "  철수 - 이름\n",
        "  \n",
        "  영희 - 이름\n",
        "  \n",
        "  밥 - 사물"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCO6j5RLoRjf"
      },
      "source": [
        "### 개체명 인식 - NLTK\n",
        "\n",
        "- https://wikidocs.net/30682\n",
        "- `nltk` 라이브러리에서는 미리 학습된 개체명 인식 모델을 제공한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyzBphuTnpVA",
        "outputId": "a347cd1a-ac41-4e9b-e4a4-d69844329ddc"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7RL6_51pJFu"
      },
      "source": [
        "**토큰화 및 품사 태깅**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsBjOb08pFVK",
        "outputId": "c040821f-362a-4bdb-a62a-962978371292"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "sentence = \"James is working at Diseny in London\"\n",
        "sentence = pos_tag(word_tokenize(sentence))\n",
        "sentence"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('James', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('working', 'VBG'),\n",
              " ('at', 'IN'),\n",
              " ('Diseny', 'NNP'),\n",
              " ('in', 'IN'),\n",
              " ('London', 'NNP')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnPYdjodpsns"
      },
      "source": [
        "**개체명 인식**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74hpIASNpYk1",
        "outputId": "c380bfce-2a96-460d-9996-2b370110fbd1"
      },
      "source": [
        "sentence = ne_chunk(sentence)\n",
        "print(sentence)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON James/NNP)\n",
            "  is/VBZ\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Diseny/NNP)\n",
            "  in/IN\n",
            "  (GPE London/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiN464LUqJqF"
      },
      "source": [
        "### 개체명 인식 - LSTM\n",
        "\n",
        "- https://wikidocs.net/24682\n",
        "- 직접 개체명 인식 모델을 구성해 학습하고 사용할 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYKghcPGqEym"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKHahTYrqv2c"
      },
      "source": [
        "- 공개된 개체명 인식 데이터셋을 이용\n",
        "  - https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt\n",
        "- 해당 데이터는 단어-개체명 형식으로 이루어져 있으므로 이를 가공해 데이터셋을 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAyT8IG9qtDl",
        "outputId": "fc575c63-194e-4620-f6c2-964d4088d737"
      },
      "source": [
        "tagged_sentences = []\n",
        "sentence = []\n",
        "\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt\") as f:\n",
        "    for line in f:\n",
        "        line = line.decode('utf-8')\n",
        "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
        "            if len(sentence) > 0:\n",
        "                tagged_sentences.append(sentence)\n",
        "                sentence = []\n",
        "            continue\n",
        "        splits = line.strip().split(' ')\n",
        "        word = splits[0].lower()\n",
        "        sentence.append([word, splits[-1]])\n",
        "\n",
        "tagged_sentences[:3]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['eu', 'B-ORG'],\n",
              "  ['rejects', 'O'],\n",
              "  ['german', 'B-MISC'],\n",
              "  ['call', 'O'],\n",
              "  ['to', 'O'],\n",
              "  ['boycott', 'O'],\n",
              "  ['british', 'B-MISC'],\n",
              "  ['lamb', 'O'],\n",
              "  ['.', 'O']],\n",
              " [['peter', 'B-PER'], ['blackburn', 'I-PER']],\n",
              " [['brussels', 'B-LOC'], ['1996-08-22', 'O']]]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6PMQSSFvBKN"
      },
      "source": [
        "**데이터 전처리**\n",
        "\n",
        "- 단어와 개체명 태그를 분리해서 데이터를 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL1WlpiQt1nm",
        "outputId": "ccb34ef1-da93-4a2d-9c31-50781d7faf0a"
      },
      "source": [
        "sentences, ner_tags = [], []\n",
        "\n",
        "for tagged_sentence in tagged_sentences:\n",
        "    sentence, tag_info = zip(*tagged_sentence)\n",
        "    sentences.append(list(sentence)) # 단어 정보만\n",
        "    ner_tags.append(list(tag_info))  # 태그 정보만\n",
        "\n",
        "print(sentences[0])\n",
        "print(tag_info[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
            "B-ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufZbGTiMvyH8"
      },
      "source": [
        "**정제 및 빈도 수가 높은 상위 단어들만 추출하기 위해 토큰화 작업**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zATPO-8TvRCW"
      },
      "source": [
        "max_words = 4000\n",
        "src_tokenizer = Tokenizer(num_words=max_words, oov_token='OOV')\n",
        "src_tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(ner_tags)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8SWg53GxEbQ",
        "outputId": "8242a1cc-6f0c-4b4f-edfc-049f753d2ae1"
      },
      "source": [
        "vocab_size = max_words\n",
        "tag_size = len(tar_tokenizer.word_index) + 1\n",
        "\n",
        "print(vocab_size)\n",
        "print(tag_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgnEFDK9xOoV"
      },
      "source": [
        "**데이터를 학습에 활용하기 위해 데이터를 배열로 변환**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x36FJwJUxLp6"
      },
      "source": [
        "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
        "y_train = tar_tokenizer.texts_to_sequences(ner_tags)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXVK56aMxifs"
      },
      "source": [
        "- 학습에 투입할 때는 동일한 길이를 가져야 하므로, 지정해둔 최대 길이에 맞춰 모든 데이터를 동일한 길이로 맞춘다.\n",
        "- 일반적으로 길이를 맞출 때는 모자란 길이만큼 0을 추가한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SY7OtcRxhxE"
      },
      "source": [
        "max_len = 70\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
        "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo8sZwwfz-zQ"
      },
      "source": [
        "**훈련, 테스트 데이터 분리 및 원핫 인코딩**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p-WwRH2z8-8"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=111)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = to_categorical(y_test, num_classes=tag_size)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJv5uISP0Tdg",
        "outputId": "f4427864-f094-4f83-8bf7-5baa3d31c800"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11232, 70)\n",
            "(11232, 70, 10)\n",
            "(2809, 70)\n",
            "(2809, 70, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viJq7M680eEo"
      },
      "source": [
        "**모델 구축 및 학습**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUpYKZLN0ad5"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FabAdsqw0ukK"
      },
      "source": [
        "**모델의 구성**\n",
        "1. 입력을 벡터로 임베딩\n",
        "2. 양방향 LSTM 구성\n",
        "3. Dense layer를 통한 각 태그에 속할 확률 예측\n",
        "\n",
        "`TimeDistributed`는 상위 layer의 출력이 step에 따라 여러 개로 출력되어 이를 적절하게 분배해주는 역할을 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuLUCeAY0sQW",
        "outputId": "18336607-3760-427a-8384-4d5c44ceeccf"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 70, 128)           512000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 70, 512)           788480    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 70, 10)            5130      \n",
            "=================================================================\n",
            "Total params: 1,305,610\n",
            "Trainable params: 1,305,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMOO-7bQ1Yym",
        "outputId": "dad72dec-b707-44f0-f87a-dd8f6ef99f5b"
      },
      "source": [
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(0.001),\n",
        "    metrics=['acc']\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=3, validation_data=(X_test, y_test))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "88/88 [==============================] - 115s 1s/step - loss: 0.1896 - acc: 0.8231 - val_loss: 0.1387 - val_acc: 0.8316\n",
            "Epoch 2/3\n",
            "88/88 [==============================] - 105s 1s/step - loss: 0.1038 - acc: 0.8506 - val_loss: 0.0805 - val_acc: 0.8807\n",
            "Epoch 3/3\n",
            "88/88 [==============================] - 103s 1s/step - loss: 0.0665 - acc: 0.9032 - val_loss: 0.0549 - val_acc: 0.9204\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7531cc490>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8--99O51qlb",
        "outputId": "a2f71e29-69bf-4fcc-ed3c-ac8395276617"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/88 [==============================] - 12s 134ms/step - loss: 0.0549 - acc: 0.9204\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05490933731198311, 0.9203764200210571]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3k-kAC93Hm-"
      },
      "source": [
        "**학습한 모델을 통한 예측**\n",
        "\n",
        "- 예측을 확인하기 위해서 인덱스를 단어로 변환해줄 사전이 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c6sCgdl3ACp"
      },
      "source": [
        "idx2word = src_tokenizer.index_word\n",
        "idx2ner = tar_tokenizer.index_word\n",
        "idx2ner[0] = 'PAD'"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF9DCsq23dwV",
        "outputId": "4584a867-4098-43d4-9abd-8e3cf5c2ab21"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "i = 10\n",
        "\n",
        "y_predicted = model.predict(np.array([X_test[i]]))\n",
        "y_predicted = np.argmax(y_predicted, axis=-1)\n",
        "true = np.argmax(y_test[i], -1)\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"에측값\"))\n",
        "print(\"-\" * 35)\n",
        "\n",
        "for w, t, pred in zip(X_test[i], true, y_predicted[0]):\n",
        "    if w != 0:\n",
        "        print(\"{:17}: {:7} {}\".format(idx2word[w], idx2ner[t].upper(), idx2ner[pred].upper()))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어             |실제값  |에측값\n",
            "-----------------------------------\n",
            "OOV              : B-PER   B-ORG\n",
            "ballanger        : I-PER   B-LOC\n",
            "of               : O       O\n",
            "france           : B-LOC   B-LOC\n",
            "confirmed        : O       O\n",
            "her              : O       O\n",
            "status           : O       O\n",
            "as               : O       O\n",
            "the              : O       O\n",
            "world            : O       O\n",
            "'s               : O       O\n",
            "number           : O       O\n",
            "one              : O       O\n",
            "woman            : O       O\n",
            "OOV              : O       O\n",
            "when             : O       O\n",
            "she              : O       O\n",
            "OOV              : O       O\n",
            "her              : O       O\n",
            "title            : O       O\n",
            "at               : O       O\n",
            "the              : O       O\n",
            "world            : O       O\n",
            "cycling          : O       O\n",
            "championships    : O       O\n",
            "on               : O       O\n",
            "friday           : O       O\n",
            ".                : O       O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGUOkzJb4e9v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}